\usepackage{amsmath}
---
title: "Group 2"
author:
  - Silver Lee-A-Fong
  - Jakub Lewkowicz
  - Domantas Pukeleviƒçius
date: "2023-03-02"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Exercise 1. Trees
#### a)
In treeVolume data set we can distinguish response variable "Volume" and explanatory variables such as "type", "height" and "diameter". We performed ANOVA with response variable "Volume" and only one explanatory variable "type" to analyse if type of a tree impacts its volume. Based on p-value from ANOVA table, we can observe that tree volume is not significantly impacted by a tree type.
```{r}
df <- read.csv("Data/treeVolume.txt",header = TRUE, sep = "")
df$type = as.factor(df$type)
anova_m = lm(volume ~ type, data=df);anova(anova_m)
```
We split data into tree categories and compared means of those two sample groups by performing t-test. Such t-test can be related to ANOVA performed above, because it also compares significant difference between volumes in those two groups. Summarizing t-test p-value implies that volume is not impacted by type of a tree.

```{r}
beech_samples = df[df$type == "beech", "volume"]
oak_samples = df[df$type == "oak", "volume"]
t.test(beech_samples, oak_samples)
```

Estimated volumes of those two tree types can be expressed as sample mean for a particular group and standard error.
\[\bar{X} = \frac{1}{n}\sum_{i=1}^{n}X_i\] 
\[SE = \frac{\sigma}{\sqrt{n}}\]
```{r}
beech_mean = mean(beech_samples); oak_mean = mean(oak_samples)
se_beech = sd(beech_samples) / sqrt(length(beech_samples))
se_oak = sd(oak_samples) / sqrt(length(oak_samples))
beech_mean; se_beech
oak_mean; se_oak
```

#### b)

We performed ANOVA where we included both height and diameter as explanatory variables and volume as response variable. From ANOVA table we can observe that both factors have significant impact on tree's volume.

```{r}
anova_m = lm(volume ~ height + diameter, data=df); anova(anova_m)
```
We also performed different ANOVAs on groups separated by types. In this test we wanted to test influence of diameter and height separately on those two groups. From ANOVA tables where our explanatory variable was diameter, we can observe that volume is influenced in non significantly different way for both groups. Contrary, from ANOVA tables where height was explanatory variable, we can observe that it does not influence oak's volume in a significant way. Beech type tree's height influences it's volume in a significant way.
```{r}
anova_beech_diameter = lm(beech_samples ~ df[df$type == "beech", "diameter"]); anova(anova_beech_diameter)
anova_oak_diameter = lm(oak_samples ~ df[df$type == "oak", "diameter"]); anova(anova_oak_diameter)

anova_beech_height = lm(beech_samples ~ df[df$type == "beech", "height"]); anova(anova_beech_height)
anova_oak_height = lm(oak_samples ~ df[df$type == "oak", "height"]); anova(anova_oak_height)
```

#### c)

Based on previously achieved results we can draw following conclusions:
  - Type does not impact tree's volume.
  - If we take all samples into consideration both height and diameter impact tree's volume significantly.
  - If we analyse height and diameter influence on tree's volume grouped by tree's type, we can observe that height does not influence oak's volume in a significant way.
  
We can observe that height and diameter are correlated for beech type trees, while they are not for oak type trees. This observation can also explain why height does not impact oak's volume in a significant way (what we observed previously).
```{r}
par(mfrow=c(1, 2))
plot(df[df$type == "beech", "diameter"], df[df$type == "beech", "height"])
plot(df[df$type == "oak", "diameter"], df[df$type == "oak", "height"])
cor.test(df[df$type == "beech", "diameter"], df[df$type == "beech", "height"])
cor.test(df[df$type == "oak", "diameter"], df[df$type == "oak", "height"])
```
Using the resulting model we predicted volume of a tree, based on the avaregae diameter.
```{r}
avg_diameter = mean(df$diameter)
model = lm(volume ~ diameter, data=df);
predict(model, data.frame(diameter = c(avg_diameter)))
```

#### d)
We decided to compare two different linear models. In both volume is a response variable, both have height and diameter as explanatory variable. Second model additionaly includes interaction between diameter and height, because those factors might be dependent on each other, therefore we decided to test it. P-value from ANOVA table confirms that adding interaction (between diameter and height) to a model2, significantly improves its fit over model1.
```{r}
model1 = lm(volume ~ diameter + height, data=df)
model2 = lm(volume ~ diameter + height + diameter*height, data=df)
anova(model1, model2)
```

### Exercise 2. Expenditure on criminal activities

### a)
Influence points investigation was done by calculating Cook's distances for fitted with linear regression model where expend is response variable and bad, crime, lawyers, employ and pop are explanatory variables. It was found that data points at indexes 5, 8, 35 and 44 produce Cook's distance values larger than 1, which makes these indices influence points (according to rule of thumb).

collinearity problem is depicted in paired scatter plots, where each explanatory variable is plotted against each other (pairwise collinearities). From scatter plots it is evident that these pairs of explanatory variables are correlated: bad and lawyers, bad and employ, bad and pop, lawyers and employ, lawyers and pop, employ and pop. To confirm other pairwise collinearities graphical inference is not sufficient.

```{r}
crime_df = read.csv('data/expensescrime.txt', sep = '',header = TRUE)
crime_df$state = as.factor(crime_df$state)
response_vars = c('bad','crime','lawyers','employ','pop')
crime_lm = lm(expend ~ bad + crime + lawyers + employ + pop,data = crime_df)
cooks_dist = cooks.distance(crime_lm)

plot(1:51,cooks_dist,type="b", xlab='index',main='Cooks distances')
pairs(crime_df[response_vars])

round(cooks_dist[cooks_dist > 1],2)
```

### b)

By using step-up method for variable selection it was found that $expend_i = \beta_0 + \beta_1 employ_i + \beta_2 lawyers_i + e$ model is our best choice. This model reported $R^2$ value of 0.963 and by adding other explanatory variables the $R^2$ did not increase significantly (also models with three features had atleast one insignificant predictor). Therefore, third variable is not included into our linear regression. Also, we checked that chosen model's p-values for intercept, employ and lawyers were significant ($p-value < 0.05$).

The estimated $\beta_1$ and  $\beta_2$ coefficients for employ and lawyers respectively suggest that an increase of one employee in the state increases crime related expenditures by 30, while an additional lawyer costs 27 dollars.  

Trying models with one explanatory variable:
```{r}
round(summary(lm(expend ~ bad,data=crime_df))$r.squared,3)
round(summary(lm(expend ~ bad,data=crime_df))$coeff,3)
```
```{r}
round(summary(lm(expend ~ crime,data=crime_df))$r.squared,3)
round(summary(lm(expend ~ crime,data=crime_df))$coeff,3)
```

```{r}
round(summary(lm(expend ~ lawyers,data=crime_df))$r.squared,3)
round(summary(lm(expend ~ lawyers,data=crime_df))$coeff,3)
```
```{r}
round(summary(lm(expend ~ employ,data=crime_df))$r.squared,3)
round(summary(lm(expend ~ employ,data=crime_df))$coeff,3)
```
```{r}
round(summary(lm(expend ~ pop,data=crime_df))$r.squared,3)
round(summary(lm(expend ~ pop,data=crime_df))$coeff,3)
```
we choose employ as single explanatory variable, since it has the largest $R^2$ and employ is significant.

Trying models with two explanatory variables (expend ~ employ + ?):
```{r}
round(summary(lm(expend ~ employ + bad,data=crime_df))$r.squared,3)
round(summary(lm(expend ~ employ + bad,data=crime_df))$coeff,3)
```
```{r}
round(summary(lm(expend ~ employ + crime,data=crime_df))$r.squared,3)
round(summary(lm(expend ~ employ + crime,data=crime_df))$coeff,3)
```
```{r}
round(summary(lm(expend ~ employ + lawyers,data=crime_df))$r.squared,3)
round(summary(lm(expend ~ employ + lawyers,data=crime_df))$coeff,3)
```
```{r}
round(summary(lm(expend ~ employ + pop,data=crime_df))$r.squared,3)
round(summary(lm(expend ~ employ + pop,data=crime_df))$coeff,3)
```
with second iteration of step-up method, the resulting model is selected: $expend_i = \beta_0 + \beta_1 employ_i + \beta_2 lawyers_i + e$

Trying models with three explanatory variables (expend ~ employ + lawyers + ?):
```{r}
round(summary(lm(expend ~ employ + lawyers + bad,data=crime_df))$r.squared,3)
round(summary(lm(expend ~ employ + lawyers + bad ,data=crime_df))$coeff,3)
```

```{r}
round(summary(lm(expend ~ employ + lawyers + crime,data=crime_df))$r.squared,3)
round(summary(lm(expend ~ employ + lawyers + crime,data=crime_df))$coeff,3)
```

```{r}
round(summary(lm(expend ~ employ + lawyers + pop,data=crime_df))$r.squared,3)
round(summary(lm(expend ~ employ + lawyers + pop,data=crime_df))$coeff,3)
```

### c)

Our estimated 95% prediction interval for expend in our model of choice,$expend_i = \beta_0 + \beta_1 employ_i + \beta_2 lawyers_i + e$, was [-302.9307, 647.3504]. In theory, this interval could be improved by increasing sample size, however, we do not have such luxury, therefore, the only thing we could do is to increase confidence interval (for example: 98%).

```{r}
newxstate=data.frame(bad=50,crime=5000,lawyers=5000,employ=5000,pop=5000)
predict(lm(expend ~ employ + lawyers,data=crime_df), newxstate,interval="prediction",level=0.95)
```
### d)
Apply the LASSO method to choose the relevant variables (with default parameters as in the lecture and lambda=lambda.1se). (You will need to install the R-package glmnet, which is not included in the standard distribution of R.) Compare the resulting model with the model obtained in b). (Beware that in general a new run delivers a new model because of a new train set.)


For this part we refitted linear regression model from part b (recall: $expend_i = \beta_0 + \beta_1 employ_i + \beta_2 lawyers_i + e$) with training data sample that composes of 67% of original expensecrime data set. Than we estimated mean squared error (MSE) of part b model for predictions from the test data sample, which composes of 33% of original data. We want to compare b model's MSE with a new model, for which predictors are chosen with LASSO method, where $\lambda$ is derived from cross validation. As specified in the question we used most regularized $\lambda$ with errors within one standard error of the minimum. The LASSO method suggested to use lawyers and employ as predictors which is the same as our linear model from part b, where we used step up method to choose features. However, LASSO regression resulted in a model with higher MSE than model from part b when predicting was done with identical test data samples. It is also worth to address that we set the random seed of following code to 1, in order to make our results reproducible. Without set seed, each time random sampling for train and test split give different sets of data, which also results in different results for both models. 
```{r setup, include=FALSE}
library(glmnet)
```
```{r}
set.seed(1)
y=as.double(as.matrix(crime_df$expend))
x=as.matrix(crime_df[,-c(1,2)])
train=sample(1:nrow(x),0.67*nrow(x))
x.train=x[train,]; y.train=y[train]
x.test=x[-train,]; y.test=y[-train]

lm_from_b=lm(expend ~ employ + lawyers,data=crime_df, subset=train)
predict_lm_from_b=predict(lm_from_b,newdata=crime_df[-train,])
mse_lm_from_b=mean((y.test-predict_lm_from_b)^2)

lasso = glmnet(x.train,y.train,alpha=1)
cv.lasso = cv.glmnet(x.train,y.train,alpha=1,type.measure='mse')
lasso.pred=predict(lasso,s=cv.lasso$lambda.1se,newx=as.matrix(x.test))
mse.lasso=mean((y.test-lasso.pred)^2)
round(coef(lasso,s=cv.lasso$lambda.1se),4)
paste("lasso model's MSE: ",round(mse.lasso,3))
paste("part b model's MSE: ",round(mse_lm_from_b,3))
```

### Excercise 3. Titanic

Addressing missing values in Age column by filling it with a mean Age of PClass category:
```{r}
library(dplyr)
df <- read.csv("Data/titanic.txt", header=TRUE, sep="")
sapply(df, function(x) sum(is.na(x)))
df = df%>%group_by(PClass)%>%mutate(Age=ifelse(is.na(Age),mean(Age,na.rm=T),Age))
```
#### a)

For logistic regression fit, variables Sex and PClass were set to factors. It was found that all three predictors (Age, Sex and PClass) are statistically significant (p-value<0.05). Further, The estimates of logistic regression are displayed together with their odds transformation, which is done by taking an exponent of an estimate. The odds ration suggest, that males and older people had less chance to survive the disaster of Titanic. Also, passengers in higher passenger classes were expected to survive more. 

To investigate how passenger class influences the chances of survival, a bar plot representing each class and its number of survivors was drawn. Here, it can be seen that first class travelers had the most survivors compared to other classes, in which the second by survival rate was the third class, then followed by second class passengers.

To support the claim that female and younger passengers had larger chances of survival, another bar plot was constructed, where the X axis represent age of survivors, and the Y axis shows number of survivors. Also, each bar represent the proportion of male and female survivors for a given age. Here it can be seen that the distribution of survivors by age is skewed to the left and larger proportion of bars are black (representing female survivor), which confirm the claim that younger passengers and also females had better chances to survive.

```{r}
options(digits = 3)
df$Sex = as.factor(df$Sex); df$PClass = as.factor(df$PClass)
tot = xtabs(~PClass+Age+Sex, data=df)
tot.c=xtabs(Survived~PClass+Age+Sex, data=df)
#round(tot.c/tot, 2)
tot2 = xtabs(Survived~Sex+Age, data=df)
tot_partial = xtabs(~Sex+Age, data=df)
barplot(tot2, tot_partial, xlab="age", ylab="Total Survived", legend=TRUE)

tot_pclass=xtabs(Survived~PClass, data=df)
tot_pclass_partial=xtabs(~PClass, data=df)
barplot(tot_pclass, tot_pclass_partial, xlab="Pclass", ylab="Total Survived")

# Fit a logistic regression model
titanic_model_a = glm(Survived ~ PClass + Age + Sex, data=df, family="binomial")
drop1(titanic_model_a,test='Chisq')
print('logistic regression estimates:')
summary(titanic_model_a)$coeff[,'Estimate']
print("logistic regression odds:")
exp(summary(titanic_model_a)$coeff[,'Estimate'])

```

#### b)
 Investigate the interaction of predictor Age with PClass, and the interaction of Age with Sex. From this and a), choose (and justify) a resulting model. For this model, report the estimate for the probability of survival for each combination of levels of the factors PClass and Sex for a person of age 55.
 
The interaction between predictors Age and PClass is not present, because of $p-value>0.05$ (see first output). In contrast, the interaction between Age and Sex features is present, since the interaction term's p-values is less than $0.05$. Recall, that the p-values of interaction terms test $H_0: \beta_1=\beta_2$.

Regarding model choice, it was decided to proceed with model from part a, where all predictors are included (Sex, PClass, Age). The motives to choose this model comes from the fact that its coefficients are all significant. However, it should be taken into account that being a male and being a woman passenger with a same age will have different probability of survival. As plots from part a suggest that males had much worse chances of survival. 

The predicted probabilities of survival for hypothetical data of each combination of levels of the factors PClass and Sex for a person of age 55, Shows a trend that better PClass contributes a lot to a chances of survival as 55 years old male from 1st class had very similar chances of survival as the female from 3rd class. 
```{r}
Age_pclass_iaction=glm(Survived ~ Age*PClass, data=df, family = "binomial")
anova(Age_pclass_iaction,test="Chisq")['Pr(>Chi)']
Age_sex_iaction=glm(Survived ~ Age*Sex, data=df,family = "binomial")
anova(Age_sex_iaction,test="Chisq")['Pr(>Chi)']

chosen_model = glm(Survived ~ PClass + Age + Sex, data=df, family="binomial")
drop1(chosen_model,test='Chisq')['Pr(>Chi)']

pred_df=data.frame(matrix(nrow = 0, ncol = 3))
colnames(pred_df)=c("Pclass","Sex","Age")
for (i in unique(df$PClass)){
    pred_df=rbind(pred_df, data.frame(PClass=rep(i,2),
                  Sex=c("female","male"),Age=rep(55,2)))
}
pred_df$PClass=as.factor(pred_df$PClass);pred_df$Sex=as.factor(pred_df$Sex)
pred_df$pred_survived=predict(chosen_model,newdata=pred_df,type="response")
pred_df
```


#### c)
Propose a method to predict the survival status and a quality measure for your prediction and describe how you would implement that method (you do not need to implement it).

To predict survival status we could follow machine learning approach. First we would need to divide our data into training and testing sets (for example: 70% of data for training, and the rest for testing). Then, the training set should be fit to logistic regression:
\[P(Y_k=1)= \left( \frac{ 1}{ 1 + e^{x^T_k \theta}} \right), k=1,...,N\]

Then, the estimate of $\hat{\theta}$ can be obtained by the maximum likelihood. Now, the data of predictors from the test split should be fitted to predict:   
\[\hat{P_{new}}= \left( \frac{ 1}{ 1 + e^{x^T_k \hat{\theta}}} \right)\]

The predictions of $\hat{P_{new}}$ can then be mapped to binary values of survival by choosing some threshold $p_0 \in [0,1]$, for which:
\[
    \hat{Y_{new}}= 
\begin{cases}
    1,& \text{if } \hat{P_{new}}\geq p_0\\
    0,& \text{if } \hat{P_{new}}\leq p_o
\end{cases}
\]

The quality measure for these predictions could be confussion matrix, which would check for false negatives and false positives by comparing $\hat{Y_{new}}$ with real $Y$ from the test split.

#### d)

for two contingency tables, PClass & Survived and Sex & Survived, Chi squared tests were applied, which both resulted in p-values less than 0.05. Hence, it can be concluded that factors Pclass and Sex both have a significant effect on survival status.   
```{r}
cont_pclass=xtabs(~PClass+Survived,data=df);cont_pclass
chisq.test(cont_pclass)
cont_gender=xtabs(~Sex+Survived,data=df);cont_gender
chisq.test(cont_gender)

```

#### e)
The advantage of machine learning approach is that it can yield very high prediction accuracy, however, its disadvantage is that it requires a lot computational power, especially with big quantities of data. On the other hand, contingency tables test (Chi-squared test in particular) is advantageous because it is fairly easy to conduct and it does not require hard computations. However, contingency tables test only tests the presence of an effect to response variable. Also, it can only work with factor variables and it is not suitable for predicting. 


### Excercise 4. Military coups

#### a)
Before performing Poisson regression on the full data set, variables pollib and numregim were transformed to factors since they have at most 4 distinct values (levels). It was also considered to transform columns oligarchy, parties and numelec to factors as well, but since they would have had significantly more levels and the interpretation of Poisson regression would be very complex, they were left as numeric variables. The only statistically significant variables in our estimated Poisson regression on full data set were: oligarchy, pollib and parties. The general Poisson regression is defined as $Y_{in} \sim Poisson(\lambda_{in}), \lambda_{in} = e^{\mu + \alpha_i + \beta X_{in}}$, where $i=1,...,I$, $n=1,...,N$. Thus, by interpreting only the coefficients of significant features we can say that the factor variable pollib increases $\lambda$ by $e^{pollib_1}=e^{-1.175}$ times, when $pollib=1$; $\lambda$ increases $e^{pollib2}=e^{-1.808}$ times, for $pollib=2$. Since default R parameterization assumes that the first level of factor is equal to 0, its coefficient is not in summary. Let $P_{in}$ denote random variable parties, then $\lambda$ would increase for every one-unit increase in $P_{in}$ by $e^{0.033}$ times. The same logic applies for variable oligarchy (lets denote it $O_{in}$), every one-unit increase in $O_{in}$, increases $\lambda$ by $e^{0.082}$ times. 
```{r}
df <- read.csv("Data/coups.txt", header=TRUE, sep=" "); set.seed(123)
df$pollib = as.factor(df$pollib); df$numregim = as.factor(df$numregim)
model_full = glm(miltcoup ~  pollib + numregim + parties + oligarchy + pctvote + popn + size + numelec, data=df, family = "poisson")
summary(model_full)
```
#### b)

Next following step down approach variable "numelec" is being eliminated due to the highest p-value. Variable represents total number of legislative and presidential elections.
```{r}
model = glm(miltcoup ~ oligarchy + pollib + parties + pctvote + popn + size + numregim, data=df, family = "poisson")
round(summary(model)$coeff,4)
```
Next we eliminate "numregim:
```{r}
model = glm(miltcoup ~ oligarchy + pollib + parties + pctvote + popn + size, data=df, family = "poisson")
round(summary(model)$coeff,4)
```
Next we eliminate "size":
```{r}
model = glm(miltcoup ~ oligarchy + pollib + parties + pctvote + popn, data=df, family = "poisson")
round(summary(model)$coeff,4)
```
Next we eliminate "popn":
```{r}
model = glm(miltcoup ~ oligarchy + pollib + parties + pctvote, data=df, family = "poisson")
round(summary(model)$coeff,4)
```
finally we eliminate "pctvote" ending up with the model were are explanatory variables are significant:
```{r}
model = glm(miltcoup ~ oligarchy + pollib + parties, data=df, family = "poisson")
round(summary(model)$coeff,4)
```
We can compare new model with eliminated variables and old model that uses variables by performing likelihood ratio test.
```{r}
summary(model_full); summary(model)
anova(model_full, model, test="LRT")
```
#### c)

By using the model from b) to predict the number of coups for a hypothetical country for all the three levels of political liberalization and averages of remaining characteristics (parties and oligarchy), we can clearly observe that level of political liberalization significantly impacts results. The lower political liberalization factor, the higher predicted number of coups.

```{r}
library(ggplot2)
hypothetical_df = data.frame(pollib=c(0,1,2), oligarchy=rep(mean(df$oligarchy),3),
           parties=rep(mean(df$parties),3))
hypothetical_df$pollib = as.factor(hypothetical_df$pollib)
hypothetical_df$pred_miltcoup = predict(model, newdata=hypothetical_df, type="response")
plot(x = hypothetical_df$pollib,y=hypothetical_df$response,
     )
p = ggplot(hypothetical_df, aes(x=pollib, y=pred_miltcoup)) + geom_point()
p + xlab("pollib") + ylab("predicted miltcoup")
```
