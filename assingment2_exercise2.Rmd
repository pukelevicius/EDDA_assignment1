---
title: "assignment2_edda"
output: pdf_document
date: "2023-03-01"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
The data in expensescrime.txt Download expensescrime.txt were obtained to determine factors related to state expenditures on criminal activities (courts, police, etc.) The variables are: state (indicating the state in the USA), expend (state expenditures on criminal activities in $1000), bad (crime rate per 100000), crime (number of persons under criminal supervision), lawyers (number of lawyers in the state), employ (number of persons employed in the state) and pop (population of the state in 1000). In the regression analysis, take expend as response variable and bad, crime, lawyers, employ and pop as explanatory variables


Make some graphical summaries of the data. Investigate the problem of influence points, and the problem of collinearity.

Influence points investigation was done by calculating Cook's distances for fitted with linear regression model where expend is response variable and bad, crime, lawyers, employ and pop are explanatory variables. It was found that data points at indexes 5, 8, 35 and 44 produce Cook's distance values larger than 1, which makes these indices influence points (according to rule of thumb).

collinearity problem is depicted in paired scatter plots, where each explanatory variable is plotted against each other (pairwise collinearities). From scatterplots it is evidant that these pairs of explanaotry variables are correlated: bad and lawyers, bad and employ, bad and pop, lawyers and employ, lawyers and pop, employ and pop. To comfirm other pairwise collinearities graphical inference is not sufficient.

```{r}
crime_df = read.csv('data/expensescrime.txt', sep = '',header = TRUE)
crime_df$state = as.factor(crime_df$state)
response_vars = c('bad','crime','lawyers','employ','pop')
crime_lm = lm(expend ~ bad + crime + lawyers + employ + pop,data = crime_df)
cooks_dist = cooks.distance(crime_lm)

plot(1:51,cooks_dist,type="b", xlab='index',main='Cooks distances')
pairs(crime_df[response_vars])

round(cooks_dist[cooks_dist > 1],2)

```

b)  Fit a linear regression model to the data. Use the step-up method to find the best model. Comment.


By using step-up method for variable selection it was found that $expend = \beta_0 + \beta_1 employ + e$ model is our best choice. This model reported $R^2$ value of 0.954 and by adding any other explanatory variable the $R^2$ did not increase significantly, therefore, we do not include second variable into our linear regression. Also, we checked that chosen model's p-values for intercept and employ were significant ($p-value < 0.05$). 
```{r}
#with one var (expend ~ ?)
round(summary(lm(expend ~ bad,data=crime_df))$r.squared,3)
round(summary(lm(expend ~ crime,data=crime_df))$r.squared,3)
round(summary(lm(expend ~ lawyers,data=crime_df))$r.squared,3)
round(summary(lm(expend ~ employ,data=crime_df))$r.squared,3) #largest R^2
round(summary(lm(expend ~ pop,data=crime_df))$r.squared,3)
#check significance:
round(summary(lm(expend ~ employ,data=crime_df))$coefficients,3)

#with 2 vars (expend ~ employ + ?)
round(summary(lm(expend ~ employ + bad,data=crime_df))$r.squared,3)
round(summary(lm(expend ~ employ + crime,data=crime_df))$r.squared,3)
round(summary(lm(expend ~ employ + lawyers,data=crime_df))$r.squared,3) #largest R^2
round(summary(lm(expend ~ employ + pop,data=crime_df))$r.squared,3)

```
c)  Determine a 95% prediction interval for the expend using the model you preferred in b) for a (hypothetical) state with bad=50, crime=5000, lawyers=5000, employ=5000 and pop=5000. Can you improve this interval?

Our estimated 95% prediction interval for expend in our model of choice,$expend = \beta_0 + \beta_1 employ + e$, was [-406.9165, 641.6441]. In theory, this interval could be improved by increasing sample size, however, we do not have such luxury, therefore, the only thing we could do is to increase confidence interval (for example: 98%).


```{r}
newxstate=data.frame(bad=50,crime=5000,lawyers=5000,employ=5000,pop=5000)
predict(lm(expend ~ employ,data=crime_df), newxstate,interval="prediction",level=0.95)
```
d)  Apply the LASSO method to choose the relevant variables (with default parameters as in the lecture and lambda=lambda.1se). (You will need to install the R-package glmnet, which is not included in the standard distribution of R.) Compare the resulting model with the model obtained in b). (Beware that in general a new run delivers a new model because of a new train set.)


For this part we refitted linear regression model from part b (recall: $expend = \beta_0 + \beta_1 employ + e$) with training data sample that composes of 67% of original expensecrime data set. Than we computed mean squared error (MSE) of part b model for prediction from the test data sample  which composes of 33% of original data. We want to compare b model's MSE with a new model for which regressors where chosen by implemenitng LASSO method with $\lambda$ derived from cross validation. As specified in the question we used most regularized $\lambda$ with errors within one standard error of the minimum. The LASSO method suggess to use lawyers and employ as regressors which differs from our model from part b (with regressor - expense) where we used step up method to choose features. However, LASSO suggested features resulted in a regression model with higher MSE than model from part b when predicting was done with identical test data samples. Recalling the scatter plots from part a, we see that variables employ and lawyers are correlated, therefore LASSO suggested features introduce collinearity between explanatory variables. 
```{r setup, include=FALSE}
library(glmnet)
```
```{r}

y=as.double(as.matrix(crime_df$expend))
x=as.matrix(crime_df[,-c(1,2)])
train=sample(1:nrow(x),0.67*nrow(x))
x.train=x[train,]; y.train=y[train]
x.test=x[-train,]; y.test=y[-train]

lm_from_b=lm(expend ~ employ,data=crime_df, subset=train)
predict_lm_from_b=predict(lm_from_b,newdata=crime_df[-train,])
mse_lm_from_b=mean((y.test-predict_lm_from_b)^2); mse_lm_from_b # prediction quality by the linear model

lasso = glmnet(x.train,y.train,alpha=1)
cv.lasso = cv.glmnet(x.train,y.train,alpha=1,type.measure='mse')


plot(lasso,label=T,xvar='lambda')
plot(cv.lasso) # the best lambda by cross-validation
#lambda.min=cv.lasso$lambda.min; lambda.1se=cv.lasso$lambda.1se
#coef(lasso,s=cv.lasso$lambda.min)
coef(lasso,s=cv.lasso$lambda.1se)
 # cyl,hp,wt,am and carb are relevant

#lasso.pred1=predict(lasso,s=lasso$lambda.min,newx=x.test) 
lasso.pred2=predict(lasso,s=cv.lasso$lambda.1se,newx=as.matrix(x.test))

#mse1.lasso=mean((y.test-lasso.pred1)^2); mse1.lasso
mse2.lasso=mean((y.test-lasso.pred2)^2)

mse2.lasso
mse_lm_from_b


```


